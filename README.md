![Pandas Handbook](res/pandas-handbook.png)

<h1 align="center">
    ğŸ¼ Pandas Handbook
</h1>

Welcome to the **Pandas Handbook**! 
This project is a collection of [Jupyter notebooks](notebooks) and corresponding [Python scripts](src) 
designed to help you learn and master the [pandas](https://pandas.pydata.org/) library for data analysis and manipulation in Python. 
Each notebook covers a core aspect of working with pandas, with hands-on examples and practical workflows.

---

## Table of Contents
- [Intro](#intro)
- [Getting Started](#getting-started)
- [The Pandas Handbook](#the-pandas-handbook)
  - [00 General](#00-general)
  - [01 Data Structures](#01-data-structures)
  - [02 Importing & Exporting Data](#02-importing--exporting-data)
  - [03 Data Inspection](#03-data-inspection)
  - [04 Data Selection](#04-data-selection)
  - [05 Data Cleaning](#05-data-cleaning)
  - [06 Data Modifying](#06-data-modifying)
  - [07 Data Combining](#07-data-combining)
  - [08 Data Analyzing](#08-data-analyzing)
  - [09 Dates & Time Series](#09-dates--time-series)
  - [10 Plotting & Visualization](#10-plotting--visualization)
- [Who this is for](#who-this-is-for)
- [Datasets used](#datasets-used)
- [Contributing](#contributing)
- [License](#license)

---

## Intro
The **Pandas Handbook** is a practical, example-driven guide to mastering the pandas library for data analysis and manipulation in Python. 
Whether youâ€™re just starting out or looking to deepen your understanding, 
this collection of Jupyter notebooks walks you through the essential workflows and features of pandas 
using real-world datasets and clear, concise explanations. 
Each notebook builds on the last, helping you learn pandas step by stepâ€”from loading and 
inspecting data to cleaning, transforming, analyzing, and visualizing it. 
Perfect for students, data enthusiasts, and professionals alike!

---

## Getting Started
1. **Clone this repository:**
   ```bash
   git clone https://github.com/Pymetheus/pandas-handbook.git
   ```

2. **Install requirements:**
   ```bash
   cd pandas-handbook/dep
   pip install -r requirements.txt
   ```
3. **Start Jupyter Notebook:**
   ```bash
   cd pandas-handbook/notebooks    
   jupyter notebook
   ```
   Open the desired notebook in your browser and follow along!

---

## The Pandas Handbook

### 00 General
Introduces the pandas library, how to install it, and how to start using it in your Python projects.  

```pip install pandas``` â€“ Install pandas using [pip](https://pypi.org/project/pandas/).  
```import pandas as pd``` â€“ Pandas is commonly imported as pd to make code shorter and easier to read.  
```print(pd.__version__)``` â€“ Check the installed pandas version.  
```help(pd.DataFrame)``` â€“ Learn how pandas functions and objects work.

ğŸ‘‰ Check out more in the [Jupyter notebook](notebooks/00-general.ipynb) or [Python Script](src/00_general.py).  

---
### 01 Data Structures
Explains the core pandas data structuresâ€”Series and DataFrameâ€”and their underlying index types.  

```pd.Series(data=[VALUE, VALUE, VALUE], index=[1, 2, 3])``` â€“ Create a simple Series with custom index labels.  
```pd.DataFrame(data={'KEY': ['VALUE', 'VALUE', 'VALUE']}, index=[1, 2, 3])``` â€“ Create a DataFrame with a custom row index.

ğŸ‘‰ Check out more in the [Jupyter notebook](notebooks/01-data-structures.ipynb) or [Python Script](src/01_data_structures.py).  

---
### 02 Importing & Exporting Data
Shows how to read data from and write data to various file formats (CSV, Excel, JSON, etc.) with pandas.  

#### ğŸ“„ From and to CSV
```pd.read_csv(import_path)``` â€“ Read data from a CSV file.  
```df.to_csv(export_path)``` â€“ Write the DataFrame to a CSV file.  

#### ğŸ“„ From and to Excel
```pd.read_excel(import_path, index_col='INDEX_COLUMN')``` â€“ Read data from an Excel file and set the index.  
```df.to_excel(export_path)``` â€“ Export the DataFrame to an Excel file. 

#### ğŸ“„ From and to JSON
```pd.read_json(import_path)``` â€“ Read structured data from a JSON file into a DataFrame.  
```df.to_json(export_path)``` â€“ Export the DataFrame to a JSON file.  

#### ğŸ“„ From and to SQL
```pd.read_sql(sql_table, engine, index_col='INDEX_COLUMN')``` â€“ Read a SQL table into a DataFrame and set the index.  
```df.to_sql(sql_table, engine, if_exists='replace')``` â€“ Write the DataFrame to the SQL table, replacing it if it already exists.  

ğŸ‘‰ Check out more in the [Jupyter notebook](notebooks/02-import-export.ipynb) or [Python Script](src/02_import_export.py).  

---
### 03 Data Inspection
Teaches techniques to explore, summarize, and inspect datasets to better understand their structure and contents.  

#### ğŸ“Œ Basic Dataset Overview
`df.head()` â€“ Displays the first 5 rows of the DataFrame.  
`df.tail()` â€“ Displays the last 5 rows of the DataFrame.  
`df.shape` â€“ Returns the number of rows and columns as a tuple.  
`df.columns` â€“ Lists the column names in the DataFrame.  
`df.index` â€“ Shows the index (row labels) of the DataFrame.  
`df.dtypes` â€“ Displays the data type of each column.  
`df.info()` â€“ Provides a concise summary of the DataFrame, including column types, non-null counts, and memory usage.  
`df.memory_usage()` â€“ Returns memory usage of each column, useful for optimization.  

#### ğŸ” Missing & Null Values
`df.isnull().sum()` â€“ Shows the number of missing values (`NaN`) per column.  
`df.isna().sum()` â€“ Shows the number of missing values (`NaN`) per column.  
`df['COLUMN'].count()` â€“ Counts non-null values in the specified `'COLUMN'`.  
`df.notnull()` â€“ Returns a DataFrame of the same shape indicating whether each value is not null (`True`) or null (`False`).  

#### ğŸ§¬ Column-Level Inspection
`df['COLUMN'].value_counts()` â€“ Returns a count of unique values in the specified `'COLUMN'`, sorted in descending order.  
`(df['COLUMN'].str.strip() == '').sum()` â€“ Returns a count of empty strings in the specified `'COLUMN'`.  
`df['COLUMN'].apply(len)` â€“ Applies the `len()` function to each value in the specified `'COLUMN'`.  
`df['COLUMN'].unique()` â€“ Lists unique values in the specified `'COLUMN'`.  
`df['COLUMN'].nunique()` â€“ Returns the number of unique values in the specified `'COLUMN'`.  
`df['COLUMN'].sample()` â€“ Randomly samples one or more rows from the specified `'COLUMN'`.  

#### ğŸ—‚ï¸ Sorting & Ordering
`df.sort_values('COLUMN')` â€“ Sorts the DataFrame by values in the specified `'COLUMN'`.  
`df.sort_index()` â€“ Sorts the DataFrame by its index values.  

#### ğŸ“Š Descriptive Statistics
`df.describe()` â€“ Generates descriptive statistics for numeric columns.  
`df.mean()` â€“ Calculates the mean of the specified data frame values.  
`df.median()` â€“ Calculates the median of the specified data frame values.  
`df.std()` â€“ Computes the standard deviation of the specified data frame values.  
`df.quantile()` â€“ Returns the 0.5 quantile (default is median).  
`df.sum()` â€“ Returns the sum of the specified data frame values.  
`df.cumsum()` â€“ Computes the cumulative sum of the specified data frame values.  
`df.var()` â€“ Calculates the variance of the specified data frame values.  
`df.min()` â€“ Returns the minimum value in specified data frame.  
`df.max()` â€“ Returns the maximum value in specified data frame.  
`df.nlargest(n, 'COLUMN')` â€“ Returns the top `n` rows with the largest values in the specified 'COLUMN'.  
`df.nsmallest(n, 'COLUMN')` â€“ Returns the top `n` rows with the smallest values in the specified 'COLUMN'.

ğŸ‘‰ Check out more in the [Jupyter notebook](notebooks/03-data-inspection.ipynb) or [Python Script](src/03_data_inspection.py).  

---
### 04 Data Selection
Demonstrates ways to select, filter, and slice data in pandas, including label- and position-based indexing.  

#### ğŸ¯ Select Data by Label or Position
`df.set_index('COLUMN', inplace=True)` â€“ Sets the specified `'COLUMN'` as the DataFrameâ€™s new index.  
`df.reset_index(inplace=True)` â€“ Resets the index to default integers and moves the current index back to a column.  

`df.iloc[row_index, column_index]` â€“ Selects data by **integer position** for rows and columns.  
`df.iat[row_index, column_index]` â€“ Accesses a **single value** by position.  
`df.loc[index_label, column_label]` â€“ Selects data by **label** for index and columns.  
`df.at[index_label, column_label]` â€“ Accesses a **single value** by label.  

Use `df.loc[]` and `df.iloc[]` to select rows and columns by label or position.  
Use `df.at[]` and `df.iat[]` for fast access to a single value.  

`df.iloc[[row1, row2], [col1, col2]]` â€“ Selects specific rows and columns by **position**.  
`df.loc[[index_label1, index_label2], ['col1', 'col2']]` â€“ Selects specific index and columns by **label**.  

#### ğŸ”¢ Slice by Row Position or Column Name
`df[start:stop]` â€“ Slices rows by position (stop is exclusive).  
`df['COLUMN']` â€“ Selects a single column by name (returns a Series).  
`df[['COLUMN_1', 'COLUMN_2']]` â€“ Selects multiple columns by name.  
`df.get(['COLUMN_1', 'COLUMN_1'])` â€“ Selects safely multiple columns by name.  
`df.COLUMN` â€“ Dot notation for selecting a column (if name is a valid Python identifier).  

#### ğŸ·ï¸ Slice by Label with `df.loc[]`
`df.loc[start_label:end_label, 'COLUMN_1':'COLUMN_3']` â€“ Slices rows and columns by label.  
`df.loc[:, ['COLUMN_1', 'COLUMN_2']]` â€“ Selects all rows, and specific columns by name.  

#### ğŸ” Select with Conditions
`df.loc[:, df.columns.str.contains('pattern')]` â€“ Selects all columns whose name matches the regex `'pattern'`.  
`df[df['COLUMN'].isin(['VALUE_1', 'VALUE_2'])]` â€“ Filters rows where the column value is in the given list.   
`df[df['COLUMN'] == 'VALUE']` â€“ Selects rows where the specified column matches the condition.    
`df[df['COLUMN'] > NUM]` â€“ Filters rows based on numeric comparison.    

#### ğŸ” Select with Query
`df.query("COLUMN == 'VALUE' and COLUMN > num")` â€“ Filters rows using a SQL-like expression with improved readability.  
`df.query("COLUMN in ['VALUE_1', 'VALUE_2']")` â€“ Filters rows where the column matches a list of values.  

#### ğŸ” Select with Regex
`df.filter(regex='pattern')` â€“ Selects columns whose names match the given regular expression.

ğŸ‘‰ Check out more in the [Jupyter notebook](notebooks/04-data-selection.ipynb) or [Python Script](src/04_data_selection.py).  

---
### 05 Data Cleaning
Covers methods for identifying and dealing with missing, invalid, or inconsistent data in real-world datasets.  

#### ğŸ§¼ Inspecting Missing Data  
```df.isna()``` â€“ Returns a DataFrame of the same shape indicating where values are `NaN` (True = missing).   
```df[df['COLUMN'].isna()]``` â€“ Filters rows where the specified column has missing values.   
```df.isna().sum()``` â€“ Counts missing values per column.  
```df.isna().sum().sum()``` â€“ Total count of missing values in entire DataFrame.   
```df[df.isna().any(axis=1)]``` â€“ Filters rows with *any* missing values.   
```df[df.isna().all(axis=1)]``` â€“ Filters rows where *all* columns are missing.   
```pd.isna(value)``` â€“ Checks if a scalar value is NaN.   
```pd.notna(value)``` â€“ Checks if a scalar value is *not* NaN.   

#### ğŸ“¥ Handling Missing Data on Import
```pd.read_csv(filepath, keep_default_na=False)``` â€“ Disables automatic conversion of certain strings (e.g., "NA", "NaN") to `NaN`.  
```pd.read_csv(filepath, na_values=['val1', 'val2'])``` â€“ Specifies additional strings to treat as `NaN` during import.  
```df.replace('VALUE', np.nan, inplace=True)``` â€“ Replaces specific values with `NaN` in-place.  

#### ğŸ§½ Cleaning Data Types
```df.dtypes``` â€“ Shows data types of each column.  
```df.select_dtypes(include=['type'])``` â€“ Selects columns with a specific data type.  
```df.select_dtypes(exclude=['type'])``` â€“ Excludes columns of a specific data type.  
```df['COLUMN'] = df['COLUMN'].astype('type')``` â€“ Converts a column to the specified data type (e.g., `'int'`, `'float'`, `'object'`).  
```pd.to_numeric(df['COLUMN'])``` â€“ Converts values to numbers.   

#### ğŸ—‘ï¸ Dropping missing or unwanted data
```df.drop(index=LABEL)``` â€“ Drops a row by index label.  
```df.dropna(axis=0)``` â€“ Drops rows with *any* missing values.  
```df.dropna(axis=1)``` â€“ Drops columns with *any* missing values.  
```df.dropna(how='all')``` â€“ Drops rows/columns only if *all* values are missing.  
```df.dropna(subset=['COLUMN_1', 'COLUMN_2'], how='any')``` â€“ Drops rows where *any* specified columns are missing.  
```df.drop(columns=['COLUMN_1', 'COLUMN_2'])``` â€“ Drops specified columns.  
```df.drop_duplicates(inplace=True)``` â€“ Removes duplicate rows in-place.  
```filter``` = ```df[df['COLUMN'] != value]``` â€“ Filters rows where a column is *not* equal to a value.  
```df.drop(index=df[filter].index)```  â€“ Drops rows which match the filter

#### ğŸ§¯ Filling Missing Data  
```df.fillna(0)``` â€“ Replaces all missing values with `0`.  
```df.fillna({'COLUMN': VALUE})``` â€“ Replaces missing values only in specified column(s).  
```df['COLUMN'].fillna(df['COLUMN'].mean())``` â€“ Fills missing values in a column with the mean.  
```df['COLUMN'].fillna(df['COLUMN'].median())``` â€“ Fills missing values with the median.  
```df.groupby(['COLUMN'])['TARGET_COLUMN'].transform(lambda x: x.fillna(x.median()))``` â€“ Fills missing values with grouped medians.  

#### ğŸ§¯ Detecting and Cleaning Invalid Categorical Values
```df['COLUMN'].str.strip().str.lower().str.title()``` â€“ Strips, lowers and titles data in a column.  
```df['COLUMN'].replace('invalid', np.nan, inplace=True)``` â€“ Replaces invalid values with `NaN`.  
```df['COLUMN'] = df['COLUMN'].replace(r'^\s*$', np.nan, regex=True)``` â€“ Replaces empty strings or whitespaces with `NaN`.

ğŸ‘‰ Check out more in the [Jupyter notebook](notebooks/05-data-cleaning.ipynb) or [Python Script](src/05_data_cleaning.py).  

---
### 06 Data Modifying
Focuses on how to update, transform, and manipulate columns and values in your DataFrames.  

#### â• Inserting or Dropping Columns
```df.insert(POSITION, 'NEW_COLUMN', df['COLUMN1'] + df['COLUMN2'])``` â€“ Inserts a new column at a specified position by summing two existing columns.  
```df.insert(POSITION, 'NEW_COLUMN', df['COLUMN'] < VALUE)``` â€“ Inserts a new column based on a condition.  
```df.drop(columns='COLUMN', inplace=True)``` â€“ Drops a single specified column in place.  
```df.drop(columns=['COLUMN_1', 'COLUMN_2', ...], inplace=True)``` â€“ Drops multiple specified columns in place.

#### ğŸ”ª Splitting & Extracting Values
```df['NEW_COLUMN'] = df['COLUMN'].str.extract(r'PATTERN')``` â€“ Extracts substrings matching a regex pattern into a new column.  
```df[['NEW_COLUMN_1', 'NEW_COLUMN_2']] = df['COLUMN'].str.split('DELIMITER', n=1, expand=True)``` â€“ Splits a column by a delimiter into multiple new columns.  

#### ğŸ”— Merging Columns
```df['NEW_COLUMN'] = df['COLUMN_1'] + 'SEPARATOR' + df['COLUMN_2']``` â€“ Creates a new column by concatenating two columns with a separator.  

#### âœï¸ Renaming and Formatting
```df.rename(columns={'OLD_NAME': 'NEW_NAME'}, inplace=True)``` â€“ Renames a column.  
```df.columns = ['COLUMN_1', 'COLUMN_2', 'COLUMN_3', ...]``` â€“ Sets new column names explicitly.

#### ğŸ§µ Modifying Column Strings
```df.columns = [x.lower() for x in df.columns]``` â€“ Converts all column names to lowercase.  
```df.columns = df.columns.str.replace(" ", "_")``` â€“ Replaces spaces with underscores in column names.  
```df['COLUMN'] = df['COLUMN'].str.upper()``` â€“ Converts all values in a column to uppercase.  

#### ğŸ§  Applying Custom Functions
```df['COLUMN'] = df['COLUMN'].apply(str.lower)``` â€“ Applies the lowercase function to all entries in a column.   
```df['COLUMN'] = df['COLUMN'].apply(custom_function)``` â€“ Applies a custom function to a column.  
```df['COLUMN'] = df['COLUMN'].apply(lambda x: x.lower())``` â€“ Applies a lambda function to convert strings to lowercase.  
```df['COLUMN'] = df['COLUMN'].replace({'OLD_VALUE': 'NEW_VALUE'})``` â€“ Replaces specific values in a column according to a mapping.  
```df['COLUMN'] = df['COLUMN'].map({'OLD_VALUE': 'NEW_VALUE'})``` â€“ Maps old values to new values in a column.

#### ğŸ¯ Conditional Modifications
```filter = df['COLUMN'] == 'VALUE'``` â€“ Creates a boolean mask for rows where a column equals a value.  
```df.loc[filter, 'COLUMN_TO_MODIFY'] = 'NEW_VALUE'``` â€“ Updates values in a column based on the condition.  
```df['COLUMN'] = df['COLUMN'].where(df['COLUMN'] < VALUE, other=np.nan)``` â€“ Sets values to `NaN` where a condition is not met.  
```df['COLUMN'] = df['COLUMN'].clip(lower=LOWER_BOUND, upper=UPPER_BOUND)``` â€“ Restricts values in a column to a specified range.

#### âœï¸ Modifying by Index or Label
```df.loc[INDEX] = ['VALUE_1', 'VALUE_2', ...]``` â€“ Replaces all column values in a row at a specific index.  
```df.loc[INDEX, 'COLUMN'] = VALUE``` â€“ Updates a single cell value by index and column label.  
```df.loc[INDEX, ['COLUMN_1', 'COLUMN_2']] = [VALUE_1, VALUE_2]``` â€“ Updates multiple columns in a row by index.  
```df.at[INDEX, 'COLUMN'] = VALUE``` â€“ Assigns a single value to a specific cell identified by label-based index and column.

ğŸ‘‰ Check out more in the [Jupyter notebook](notebooks/06-data-modifying.ipynb) or [Python Script](src/06_data_modifying.py).

---
### 07 Data Combining
Explains how to concatenate, merge, and join multiple datasets using pandasâ€™ flexible tools.  

#### ğŸ”€ Concatenation
```pd.concat([df1, df2])``` â€“ Concatenates two DataFrames vertically (row-wise).  
```pd.concat([df1, df2], axis=1)``` â€“ Concatenates two DataFrames horizontally (column-wise).  

#### ğŸ”— Merging
```pd.merge(df, new_df, on='INDEX_COLUMN')``` â€“ Merges the original and new DataFrame on a shared index column.  

#### ğŸ”„ Join Types: ```inner```, ```outer```, ```left```, ```right```
```pd.merge(left_df, right_df, on='INDEX_COLUMN', how='inner')``` â€“ Performs an inner merge, keeping only matching index values.  
```pd.merge(left_df, right_df, on='INDEX_COLUMN', how='outer')``` â€“ Performs an outer merge, keeping all index values from both DataFrames.  
```pd.merge(left_df, right_df, on='INDEX_COLUMN', how='left')``` â€“ Performs a left join, keeping all index values from the left DataFrame.  
```pd.merge(left_df, right_df, on='INDEX_COLUMN', how='right')``` â€“ Performs a right join, keeping all index values from the right DataFrame.  

#### ğŸ§© Joining
```df.join(df_new, on='INDEX_COLUMN')``` â€“ Joins the new DataFrame to the original using their shared index.  

ğŸ‘‰ Check out more in the [Jupyter notebook](notebooks/07-data-combining.ipynb) or [Python Script](src/07_data_combining.py).

---
### 08 Data Analyzing
Walks through filtering, sorting, grouping, aggregating, and pivoting data for insightful analysis.  

#### ğŸ” Filtering data
```df['COLUMN'] == VALUE``` â€“ Creates a filter for rows where the column equals a specific value.  
```df.loc[FILTER, ['COLUMN_1', 'COLUMN_2']]``` â€“ Filters rows based on a filter and selects specific columns.  

#### ğŸ”½ Sorting data
```df['COLUMN'].sort_values()``` â€“ Returns a Series sorted by values in ascending order.  
```df.sort_values(by=['COLUMN_1', 'COLUMN_2'], ascending=[True, False])``` â€“ Sorts the DataFrame by multiple columns with specified sort orders.  
```df['COLUMN'].nlargest(N)``` â€“ Returns the N largest values in a column.  
```df.nlargest(N, 'COLUMN')``` â€“ Returns the N rows with the largest values in a column.  
```df.nsmallest(N, 'COLUMN')``` â€“ Returns the N rows with the smallest values in a column.  

#### ğŸ§® Aggregating  data
```df['COLUMN'].mean()``` â€“ Calculates the mean of a column.  
```df['COLUMN'].median()``` â€“ Calculates the median of a column.  
```df['COLUMN'].mode()``` â€“ Returns the mode(s) of a column.  
```df[['COLUMN_1', 'COLUMN_2']].corr()``` â€“ Computes the pairwise correlation between columns.  
```df[['COLUMN_1', 'COLUMN_2']].cov()``` â€“ Computes the pairwise covariance between columns.

#### ğŸ‘¥ Grouping data
```df.groupby(['COLUMN'])['COLUMN_2'].mean()``` â€“ Groups the DataFrame by one column and calculates the mean of another.  
```df.groupby(['COLUMN_1', 'COLUMN_2'])``` â€“ Creates a multi-level groupby object.  
```group['COLUMN'].value_counts()``` â€“ Counts unique values in each group.  
```group['COLUMN'].value_counts().loc[KEY]``` â€“ Returns value counts for a specific group key.  
```group['COLUMN'].value_counts(normalize=True)``` â€“ Calculates the relative frequencies of values in each group.  
```group['COLUMN'].median()``` â€“ Computes the median for each group.  
```group['COLUMN'].median().loc[KEY]``` â€“ Retrieves the median for a specific group key.  
```group['COLUMN'].agg(['median', 'mean', 'std', 'min', 'max'])``` â€“ Aggregates multiple statistics per group.  
```group['COLUMN'].agg(['median', 'mean']).loc[KEY]``` â€“ Aggregates selected stats and selects a specific group.  
```group['COLUMN'].apply(lambda x: x.str.contains('PATTERN').sum())``` â€“ Counts how many entries in each group match a string pattern.  
```df['COLUMN'].value_counts()``` â€“ Counts unique values in a column.  
```df['COLUMN'].count()``` â€“ Counts non-null values in a column.  
```group['COLUMN'].apply(lambda x: x.str.contains('PATTERN').sum()).loc[KEY]``` â€“ Retrieves counts for a pattern-matching condition from a specific group.  
```group['COLUMN'].count()``` â€“ Counts non-null entries in a column for each group.  
```df.groupby('COLUMN').agg({'COLUMN_1': ['mean', 'median'], 'COLUMN_2': ['mean', 'max', 'min']})``` â€“ Applies multiple aggregation functions on multiple columns grouped by a specified column.  

#### ğŸ“Š Pivoting data
```df.pivot(index='COLUMN', columns='COLUMN_2', values=['COLUMN_3'])``` â€“ Reshapes the DataFrame based on column values as new columns.  
```pd.pivot_table(df, index='COLUMN_1', columns='COLUMN_2', values='COLUMN_3', aggfunc='mean')``` â€“ Creates a pivot table using the mean as the aggregation function.  
```pd.pivot_table(df, index='COLUMN_1', columns='COLUMN_2', values=['COLUMN_3', 'COLUMN_4'], aggfunc=['mean', 'median'])``` â€“ Creates a pivot table with multiple values and aggregation functions.  

ğŸ‘‰ Check out more in the [Jupyter notebook](notebooks/08-data-analyzing.ipynb) or [Python Script](src/08_data_analyzing.py).

---
### 09 Dates & Time Series
Introduces handling of datetime data, time series analysis, and date-based operations in pandas.  

#### ğŸ“… Datetime Conversion
```pd.read_csv(PATH, parse_dates=['COLUMN'], date_format='FORMAT')``` â€“ Reads CSV and parses specified column as datetime.  
```pd.to_datetime(df['COLUMN'], format='FORMAT')``` â€“ Converts string-formatted date column to datetime.  

#### ğŸ•°ï¸ Creating Date Ranges
```pd.date_range(START_DATE, periods=NUM, freq='FREQ')``` â€“ Generates a sequence of dates with a specified frequency.  
```pd.DataFrame(data=SEQUENCE, columns=['COLUMN'])``` â€“ Creates a DataFrame from a date sequence.  

#### ğŸ•“ Accessing Time Components
```df.at_time('HH:MM')``` â€“ Filters data for a specific time of day.  
```df.between_time('START', 'END')``` â€“ Filters data between two times of day.

#### ğŸ•“ Accessing Time with .dt
```df['COLUMN'].dt.year``` â€“ Extracts the year from datetime.  
```df['COLUMN'].dt.month``` â€“ Extracts the month from datetime.  
```df['COLUMN'].dt.quarter``` â€“ Extracts the quarter of the year.  
```df['COLUMN'].dt.day``` â€“ Extracts the day of the month.  
```df['COLUMN'].dt.hour``` â€“ Extracts the hour.  
```df['COLUMN'].dt.minute``` â€“ Extracts the minute.  
```df['COLUMN'].dt.second``` â€“ Extracts the second.  
```df['COLUMN'].dt.day_name()``` â€“ Gets the day name (e.g., Monday).

#### ğŸ•“ Accessing Date Properties
```df.loc[ROW, 'COLUMN'].date()``` â€“ Extracts date part (no time).  
```df.loc[ROW, 'COLUMN'].day_name()``` â€“ Gets name of the weekday.  
```df.loc[ROW, 'COLUMN'].month_name()``` â€“ Gets name of the month.  
```df.loc[ROW, 'COLUMN'].day``` â€“ Extracts day.  
```df.loc[ROW, 'COLUMN'].week``` â€“ Extracts week number of the year.  
```df.loc[ROW, 'COLUMN'].dayofweek``` â€“ Extracts day of week as integer.  
```df.loc[ROW, 'COLUMN'].month``` â€“ Extracts numeric month.  
```df.loc[ROW, 'COLUMN'].year``` â€“ Extracts year.

#### ğŸ•“ Time Differences with Timedelta
```df['COLUMN'].min()``` â€“ Finds earliest datetime.  
```df['COLUMN'].max()``` â€“ Finds latest datetime.  
```df['COLUMN'].max() - df['COLUMN'].min()``` â€“ Computes timedelta between two dates.

### ğŸ“† Working with Period and Offsets
```pd.Period('VALUE')``` â€“ Creates a period object (e.g., year, month).  
```period.start_time``` â€“ Returns start time of the period.  
```period.end_time``` â€“ Returns end time of the period.  
```period += pd.offsets.FREQ(AMOUNT)``` â€“ Offsets a period forward or backward in time.  

#### ğŸŒ Timezones and Localization
```df['COLUMN'].dt.tz_localize('TIMEZONE')``` â€“ Localizes datetime column to a timezone.  
```df['COLUMN'].dt.tz_convert('TIMEZONE')``` â€“ Converts timezone of a datetime column.

#### ğŸ” Filtering and Slicing Dates
```df['COLUMN'] > pd.to_datetime('DATE')``` â€“ Filters rows on a datetime condition.  
```df.index > pd.Timestamp('DATE')``` â€“ Filters rows on a Timestamp condition.  
```df.loc['YEAR']``` â€“ Slices all rows for a specific year.  
```df.loc['START_DATE' : 'END_DATE']``` â€“ Slices rows between two dates.

#### ğŸ“Š Frequency Conversion, Resampling and Rolling
```df.asfreq('FREQ')``` â€“ Changes frequency without aggregation.  
```df.resample('FREQ')['COLUMN'].mean()``` â€“ Resamples data and calculates mean.  
```df['COLUMN'].rolling(window=N).mean()``` â€“ Computes rolling mean over a window.

#### ğŸ”§ Handling Missing Data with .interpolate()
```df['COLUMN'].interpolate(limit_direction='both')``` â€“ Fills missing values using interpolation.

#### ğŸ“ˆ Calculating Lagged Values and Differences
```df['COLUMN'].shift(N)``` â€“ Shifts values by N periods (e.g., previous day's value).  
```df['COLUMN'].diff()``` â€“ Calculates the difference between current and previous value.  
```df['COLUMN'].pct_change()``` â€“ Calculates percentage change from previous value.

ğŸ‘‰ Check out more in the [Jupyter notebook](notebooks/09-dates-timeseries.ipynb) or [Python Script](src/09_dates_timeseries.py).

---
### 10 Plotting & Visualization
Demonstrates how to create common visualizations such as line plots, bar charts, histograms, and scatter plots to explore and communicate data insights effectively.  

#### ğŸ“ˆ Basic Plotting Functions
```df.plot(subplots=True, title='TITLE')``` â€“ Plots each numeric column in a separate subplot with a title.  
```df.plot(kind='line', title='TITLE')``` â€“ Plots all columns as line charts with a title.  
```df[['COLUMN_1', 'COLUMN_2']].plot(kind='line', title='TITLE')``` â€“ Plots two columns as separate lines on the same plot.  
```df.plot(kind='scatter', x='COLUMN_X', y='COLUMN_Y', title='TITLE')``` â€“ Creates a scatter plot between two variables.  
```df.plot(kind='hexbin', x='COLUMN_X', y='COLUMN_Y', title='TITLE')``` â€“ Creates a hexbin density plot for two variables.  
```df[['COLUMN_1', 'COLUMN_2']].plot(stacked=True, title='TITLE')``` â€“ Plots multiple columns as a stacked area chart.  
```df['COLUMN'].plot(kind='line', title='TITLE')``` â€“ Creates a line chart of a single column.  
```df['COLUMN'].plot(kind='area', title='TITLE')``` â€“ Creates an area chart of a single column.  
```df['COLUMN'].plot(kind='bar', title='TITLE')``` â€“ Creates a bar chart of a single column.  
```df['COLUMN'].plot(kind='hist', bins=N, title='TITLE')``` â€“ Creates a histogram with N bins.  
```df['COLUMN'].plot(kind='box', title='TITLE')``` â€“ Creates a box plot for a single column.  
```df['COLUMN'].value_counts().plot(kind='pie', title='TITLE')``` â€“ Creates a pie chart from value counts of a column.

#### ğŸ¨ Basic Customizations
```df[['COLUMN_1', 'COLUMN_2']].plot(kind='KIND', title='TITLE', xlabel='LABEL', ylabel='LABEL', color=['COLOR_1', 'COLOR_2'], figsize=(WIDTH, HEIGHT), grid=True, legend=True, alpha=VALUE)``` â€“ Creates a customized line chart with labels, size, grid, colors, and transparency.

#### ğŸ”§ Plot Configuration
```df['COLUMN'].plot(title='TITLE', figsize=(WIDTH, HEIGHT), style='STYLE', color='COLOR', linewidth=VALUE, alpha=VALUE, xlim=('START', 'END'), ylim=(LOW, HIGH), rot=ANGLE)``` â€“ Creates a styled line plot with axis limits, rotation, and transparency.  
```axis2 = df['COLUMN_2'].plot(ax=axis1, secondary_y=True, ...)``` â€“ Overlays a second line with a secondary y-axis.   
```axis2.set_ylabel('LABEL')``` â€“ Sets y-axis label for the secondary axis.   
```axis1.set_ylabel('LABEL')``` â€“ Sets y-axis label for the primary axis.   
```axis1.legend(['LABEL'], loc='LOCATION')``` â€“ Adds a legend for the first plot.   
```axis1.right_ax.legend(['LABEL'], loc='LOCATION')``` â€“ Adds a legend for the second axis.   
```plt.show()``` â€“ Displays the plot.  

#### ğŸ–¼ï¸ Save Plots
```plt.savefig(export_path)``` â€“ Saves the current plot to the specified file path.

ğŸ‘‰ Check out more in the [Jupyter notebook](notebooks/10-plotting-visualization.ipynb) or [Python Script](src/10_plotting_visualization.py).


---

## Who this is for
- **Beginners** who want to learn pandas from scratch.
- **Students** in data science, machine learning, or analytics.
- **Anyone** who prefers learning by example and hands-on code.

---

## Datasets used
- [Ramen Ratings](https://www.kaggle.com/datasets/residentmario/ramen-ratings/data)
- [Titanic: Machine Learning from Disaster](https://www.kaggle.com/competitions/titanic/data)
- [Weather Prediction](https://www.kaggle.com/datasets/ananthr1/weather-prediction/data)

_Datasets should be placed in the appropriate `/data/raw/` or `/data/processed/` folders as described in each notebook._

---

## Contributing
Contributions to this project are welcome! If you would like to contribute, please open an issue to discuss potential changes or submit a pull request. 
For more details please visit the [contributing page](docs/CONTRIBUTING.md).

---

## License
This project is licensed under the [MIT License](LICENSE.md). You are free to use, modify, and distribute this code as permitted by the license.

---
 
<h4 align="center">
    Happy pandas coding! ğŸ¼
</h4>

![Happy Pandas Coding](res/pandas-coding.png)
